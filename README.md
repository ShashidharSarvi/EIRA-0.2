# ğŸ§  EIRAâ€‘0.2: Multimodal Medical QA with LLaMAâ€‘2 & BLIP

> Bridging medical imaging and natural language for clinical question answering.

EIRAâ€‘0.2 is a multimodal deep learning model that performs free-form question answering over medical images and related text. It fuses a visual encoder (BLIP) and a language model (LLaMA-2) to support context-aware, image-text-to-text inferenceâ€”ideal for clinical decision support and educational tools.

---

## ğŸ“¦ Features

- ğŸ”¬ Medical image + text â†’ free-form medical answer  
- ğŸ¤ Built on `LLaMA-2` and `BLIP`  
- ğŸ¥ Tuned on radiographs, histology slides, and clinical notes  
- ğŸ§ª Evaluated on expert-curated test sets  
- ğŸš€ Hugging Face-hosted weights (no Git LFS needed)  

---

## ğŸ–¼ï¸ Example

```python
from transformers import pipeline

# Load the multimodal QA pipeline
eira = pipeline(
    task="image-text-to-text",
    model="bockhealthbharath/Eira-0.2",
    device=0  # or -1 for CPU
)

# Inference
answer = eira({
    "image": "chest_xray.png",
    "text": "What abnormality is visible in the left lung?"
})

print("Answer:", answer[0]["generated_text"])
